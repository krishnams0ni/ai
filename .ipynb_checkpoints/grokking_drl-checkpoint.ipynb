{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa25ec79-e526-4eb3-a986-c109f2035ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a75a5-8179-4f11-af58-5ec21f505ef9",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "### Markov Poperty\n",
    "$P(S_{t+1}\\vert S_t, A_t) = P(S_{T+1}\\vert S_t, A_t, S_{t-1}, A_{t-1},\\ldots)$\n",
    "\n",
    "### Transition Function\n",
    "$p(s'\\vert s, a) = P(S_t=s'\\vert S_{t-1}=s, A_{t-1}=a)$\n",
    "\n",
    "### Reward Function\n",
    "$r(s,a)=\\mathbb{E} [R_t\\vert S_{t-1}=s, A_{t-1}=a]$<br>\n",
    "$r(s,a,s')=\\mathbb{E} [R_t\\vert S_{t-1}=s, A_{t-1}=a, S_t=s']$\n",
    "\n",
    "### Return\n",
    "$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ldots+\\gamma^{T-1}R_T$\n",
    "\n",
    "### MDP\n",
    "$\\mathcal{MDP(S,A,T,R,S_\\theta,\\gamma,H)}$<br>\n",
    "$\\mathcal{POMDP(S,A,T,R,S_\\theta,\\gamma,H,O,E)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4a39b-7718-4116-be96-e9932c2140e9",
   "metadata": {},
   "source": [
    "# Bellman Equations\n",
    "\n",
    "### State-Value Function\n",
    "$v_\\pi(s) = \\sum_a [\\pi(a|s) \\sum_{s',r} [p(s',r|s, a) \\left[r + \\gamma v_\\pi(s') \\right],\\forall s \\in S]]$\n",
    "\n",
    "### Action-Value Function\n",
    "$q_\\pi(s,a) = \\sum_{s',r} p(s',r|s, a) \\left[r + \\gamma v_\\pi(s') \\right],\\forall s \\in S,\\forall a \\in A$\n",
    "\n",
    "### Action Advantage\n",
    "$a_\\pi(s,a)=q_\\pi(s,a)-v_\\pi(s)$\n",
    "\n",
    "### Bellman optimality equations\n",
    "\n",
    "$v_\\star(s)=\\displaystyle\\max_\\pi v_\\pi(s),\\forall s \\in S$<br>\n",
    "$q_\\star(s,a)=\\sum_{s',r}p(s',r\\vert s,a)[r+\\gamma \\displaystyle\\max_{a'}q_\\star(s',a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0b631-81f6-4510-9240-df0ec9858228",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "### Policy Evaluation\n",
    "$v_{k+1}(s)=\\sum_a \\pi(a\\vert s) \\sum_{s',r}p(s',r\\vert s,a)[r+\\gamma v_k(s')]$\n",
    "\n",
    "### Policy Improvement\n",
    "$\\pi'(s)=\\text{argmax}_a \\sum_{s',r}p(s',r\\vert s,a)[r+\\gamma v_k(s')]$\n",
    "\n",
    "# Value Iteration\n",
    "$v_{k+1}(s)=\\displaystyle\\max_a\\sum_{s',r}p(s',r\\vert s,a)[r+\\gamma v_k(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac7a55-b521-4ca9-9372-8f6afdbe3001",
   "metadata": {},
   "source": [
    "# Total Regret\n",
    "\n",
    "$\\mathcal{T}=\\sum_{e=1}^E \\mathbb{E}[v_\\star-q_\\star(A_e)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe70eb6-d034-4ab5-8330-5c7e5a85a283",
   "metadata": {},
   "source": [
    "# Softmax Exploration\n",
    "\n",
    "$\\pi(a)=\\frac{exp(\\frac{Q(a)}{\\mathcal{T}})}{\\sum_{b=0}^B exp(\\frac{Q(b)}{\\mathcal{T}})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4851f959-deb2-4d96-95fb-bc2077bac073",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT, RIGHT = 0, 1\n",
    "\n",
    "P = {\n",
    "    #   state: {action: [(prob. of transition, next state, reward, if statement is terminal)]}\n",
    "    0: {\n",
    "        0: [\n",
    "            (0.5000, 0, 0.0, True),\n",
    "            (0.3333, 0, 0.0, True),\n",
    "            (0.1666, 0, 0.0, True),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 0, 0.0, True),\n",
    "            (0.3333, 0, 0.0, True),\n",
    "            (0.1666, 0, 0.0, True),\n",
    "        ],\n",
    "    },\n",
    "    1: {\n",
    "        0: [\n",
    "            (0.5000, 0, 0.0, True),\n",
    "            (0.3333, 1, 0.0, False),\n",
    "            (0.1666, 2, 0.0, False),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 2, 0.0, False),\n",
    "            (0.3333, 1, 0.0, False),\n",
    "            (0.1666, 0, 0.0, True),\n",
    "        ],\n",
    "    },\n",
    "    2: {\n",
    "        0: [\n",
    "            (0.5000, 1, 0.0, False),\n",
    "            (0.3333, 2, 0.0, False),\n",
    "            (0.1666, 3, 0.0, False),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 3, 0.0, False),\n",
    "            (0.3333, 2, 0.0, False),\n",
    "            (0.1666, 1, 0.0, False),\n",
    "        ],\n",
    "    },\n",
    "    3: {\n",
    "        0: [\n",
    "            (0.5000, 2, 0.0, False),\n",
    "            (0.3333, 3, 0.0, False),\n",
    "            (0.1666, 4, 0.0, False),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 4, 0.0, False),\n",
    "            (0.3333, 3, 0.0, False),\n",
    "            (0.1666, 2, 0.0, False),\n",
    "        ],\n",
    "    },\n",
    "    4: {\n",
    "        0: [\n",
    "            (0.5000, 3, 0.0, False),\n",
    "            (0.3333, 4, 0.0, False),\n",
    "            (0.1666, 5, 0.0, False),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 5, 0.0, False),\n",
    "            (0.3333, 4, 0.0, False),\n",
    "            (0.1666, 3, 0.0, False),\n",
    "        ],\n",
    "    },\n",
    "    5: {\n",
    "        0: [\n",
    "            (0.5000, 4, 0.0, False),\n",
    "            (0.3333, 5, 0.0, False),\n",
    "            (0.1666, 6, 1.0, True),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 6, 1.0, True),\n",
    "            (0.3333, 5, 0.0, False),\n",
    "            (0.1666, 4, 0.0, False),\n",
    "        ],\n",
    "    },\n",
    "    6: {\n",
    "        0: [\n",
    "            (0.5000, 6, 0.0, True),\n",
    "            (0.3333, 6, 0.0, True),\n",
    "            (0.1666, 6, 0.0, True),\n",
    "        ],\n",
    "        1: [\n",
    "            (0.5000, 6, 0.0, True),\n",
    "            (0.3333, 6, 0.0, True),\n",
    "            (0.1666, 6, 0.0, True),\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "pi = lambda s: {0: LEFT, 1: LEFT, 2: LEFT, 3: LEFT, 4: LEFT, 5: LEFT, 6: LEFT}[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6671769c-c29e-4628-87f2-cb280b5d19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    prev_V = np.zeros(len(P))\n",
    "    while True:\n",
    "        V = np.zeros(len(P))\n",
    "        for s in range(len(P)):\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5edbab3d-f82e-42e9-9ae4-e2ec9a965067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, P, gamma=1.0):\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "    for s in range(len(P)):\n",
    "        for a in range(len(P[s])):\n",
    "            for prob, next_state, reward, done in P[s][a]:\n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "    new_pi = lambda s: {s: a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1827ee24-06e3-462e-aa51-f900191c75c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 249.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 0, 0, 0, 0, 0]\n",
      "[0.         0.002739   0.01096093 0.03564318 0.10974096 0.33218911\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n",
      "[0.         0.66690987 0.88925762 0.96352174 0.98845925 0.99696612\n",
      " 0.        ] \n",
      " [0, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    print(V, \"\\n\", [pi(_) for _ in range(6)])\n",
    "    V = policy_evaluation(pi, P)\n",
    "    pi = policy_improvement(V, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50d5b709-3a28-4ead-83f3-e223176aaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))\n",
    "    pi = lambda s: {s: a for s, a in enumerate(random_actions)}[s]\n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))}\n",
    "        V = policy_evaluation(pi, P, gamma, theta)\n",
    "        pi = policy_improvement(V, P, gamma)\n",
    "        if old_pi == {s: pi(s) for s in range(len(P))}:\n",
    "            break\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61006af-cc75-40de-8c59-88c9cdfd2f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
